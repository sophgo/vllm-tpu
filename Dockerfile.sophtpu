# This vLLM Dockerfile is used to construct image that can build and run vLLM on Sophon TPU platform.

FROM ubuntu:22.04 AS base

ENV CCACHE_DIR=/root/.cache/ccache
ENV CMAKE_CXX_COMPILER_LAUNCHER=ccache
ENV DEBIAN_FRONTEND=noninteractive

ARG PYTHON_VERSION=3.12

# Install Python and other dependencies
RUN apt-get update -y \
    && apt-get install -y vim ccache software-properties-common git curl sudo \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update -y \
    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \
    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \
    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \
    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \
    && python3 --version && python3 -m pip --version

RUN apt-get update -y \
    && apt-get install -y vim pandoc wget numactl gcc-12 g++-12 libtcmalloc-minimal4 libnuma-dev \
    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \
    && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12 \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace

COPY requirements-common.txt requirements-common.txt
COPY requirements-sophtpu.txt requirements-sophtpu.txt
RUN pip install torch==2.8.0 torchvision==0.23 --index-url https://download.pytorch.org/whl/cpu --force-reinstall \
    && pip install setuptools_scm \
    && pip install -r requirements-sophtpu.txt \
    && rm requirements-*.txt

RUN git clone --branch v0.11.0 https://github.com/vllm-project/vllm.git \
 && cd vllm \
 && python3 use_existing_torch.py \
 && pip install -r ./requirements/build.txt \
 && VLLM_TARGET_DEVICE=empty pip install -e . --no-build-isolation

COPY vllm_sophon /workspace/vllm_sophon
WORKDIR /workspace/vllm_sophon
RUN pip install -e . --no-build-isolation

COPY third-party third-party

RUN cd third-party \
 && tar -xzvf torch-tpu*.tar.gz \
 && pip install ./dist/torch_tpu*_x86_64.whl --force-reinstall \
 && dpkg -i ./tpuv7-runtime-emulator*_amd64.deb \
 && rm -rf *

WORKDIR /workspace/
#RUN ln -s /workspace/vllm/tests && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks

# install development dependencies (for testing)
#RUN --mount=type=cache,target=/root/.cache/pip \
#    pip install -e tests/vllm_test_utils

#ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
